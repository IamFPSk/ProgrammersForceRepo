{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install contractions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDAL42S4EiUq",
        "outputId": "d2ea96ec-8108-4acc-cc36-c284fe0ee8d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "819cTBReCKoq",
        "outputId": "5092c55a-d8e2-44ba-88b8-4c59eacea3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import contractions\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Bidirectional\n",
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading CSV**"
      ],
      "metadata": {
        "id": "xiNSuna6Cd0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = pd.read_csv('/content/spam.csv', sep=',', encoding='latin-1')\n",
        "text.drop(columns=text.columns[text.columns.str.contains('unnamed', case=False)], inplace=True)\n",
        "print(text.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOTLBOfKCjgv",
        "outputId": "730ad4e0-3800-4e3a-ba03-1fa5b8a79d38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
            "6   ham  Even my brother is not like to speak with me. ...\n",
            "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
            "8  spam  WINNER!! As a valued network customer you have...\n",
            "9  spam  Had your mobile 11 months or more? U R entitle...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre processing**"
      ],
      "metadata": {
        "id": "1n-aF3XUCopF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text):\n",
        "  #Removing Contractions\n",
        "  text = contractions.fix(text)\n",
        "\n",
        "  # Removing HTML tags\n",
        "  text_cleaned = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "  # Removing URLs\n",
        "  text_cleaned = re.sub(r'http\\S+|www\\S+|https\\S+', '', text_cleaned)\n",
        "\n",
        "  # Converting to lowercase and removing non-alphanumeric characters\n",
        "  text_cleaned = re.sub(r'[^a-zA-Z\\s]', '', text_cleaned.lower())\n",
        "\n",
        "  # Tokenization\n",
        "  tokens = word_tokenize(text_cleaned)\n",
        "\n",
        "  # Removing stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens_without_sw = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Regex: Remove punctuation marks\n",
        "  tokens_without_punctuation = [word for word in tokens_without_sw if re.match(r'^\\w+$', word)]\n",
        "\n",
        "  # Lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_without_punctuation]\n",
        "\n",
        "  return \" \".join(lemmatized_tokens)\n",
        "\n",
        "text['processed_text'] = text['v2'].apply(preprocessing)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyLDigsECtBP",
        "outputId": "909fcff6-ff7e-45dd-b7d1-7ab287782242"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        v1                                                 v2  \\\n",
            "0      ham  Go until jurong point, crazy.. Available only ...   \n",
            "1      ham                      Ok lar... Joking wif u oni...   \n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
            "3      ham  U dun say so early hor... U c already then say...   \n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
            "...    ...                                                ...   \n",
            "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
            "5568   ham              Will Ì_ b going to esplanade fr home?   \n",
            "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
            "5570   ham  The guy did some bitching but I acted like i'd...   \n",
            "5571   ham                         Rofl. Its true to its name   \n",
            "\n",
            "                                         processed_text  \n",
            "0     go jurong point crazy available bugis n great ...  \n",
            "1                                 ok lar joking wif oni  \n",
            "2     free entry wkly comp win fa cup final tkts st ...  \n",
            "3                       dun say early hor c already say  \n",
            "4                   nah think go usf life around though  \n",
            "...                                                 ...  \n",
            "5567  nd time tried contact pound prize claim easy c...  \n",
            "5568                          b going esplanade fr home  \n",
            "5569                         pity mood soany suggestion  \n",
            "5570  guy bitching acted like would interested buyin...  \n",
            "5571                                     rofl true name  \n",
            "\n",
            "[5572 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making X and Y, X=Preprocessed text and Y=Labels**"
      ],
      "metadata": {
        "id": "7HVV0mcnPRc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = text['processed_text'].values\n",
        "y = np.where(text['v1'] == 'spam', 1, 0)"
      ],
      "metadata": {
        "id": "RYBIuHILE3kD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text to sequence, Padding Seq, SplitData, Word2Vec**"
      ],
      "metadata": {
        "id": "1QSWzszcPedI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text to Sequence Conversion\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Padding Sequences\n",
        "max_sequence_length = 100\n",
        "sequences_padded = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load pre-trained Word2Vec model (glove-wiki-gigaword-100)\n",
        "word2vec_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Create the embedding matrix from the Word2Vec model\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    if word in word2vec_model:\n",
        "        embedding_matrix[idx] = word2vec_model[word]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmWTPE-YPZvd",
        "outputId": "2258b9c0-cfad-4153-e5fc-9365c43347d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unidirectional RNN Model**"
      ],
      "metadata": {
        "id": "Ozw5Z6fLQfP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Unidirectional RNN Model with ReLU activation\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
        "model.add(SimpleRNN(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkgBoqjwQexs",
        "outputId": "aaaa8494-2101-45ab-cdf5-edd2cfa9f432"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "140/140 [==============================] - 7s 28ms/step - loss: 0.4274 - accuracy: 0.8661 - val_loss: 0.3949 - val_accuracy: 0.8655\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 4s 27ms/step - loss: 0.4035 - accuracy: 0.8661 - val_loss: 0.6621 - val_accuracy: 0.8655\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 5s 37ms/step - loss: 0.4631 - accuracy: 0.8661 - val_loss: 0.3918 - val_accuracy: 0.8655\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.3732 - accuracy: 0.8661 - val_loss: 0.5040 - val_accuracy: 0.8655\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 4s 26ms/step - loss: 0.3884 - accuracy: 0.8661 - val_loss: 0.3481 - val_accuracy: 0.8655\n",
            "35/35 [==============================] - 0s 8ms/step - loss: 0.3481 - accuracy: 0.8655\n",
            "Test Loss: 0.348071426153183\n",
            "Test Accuracy: 0.865470826625824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bidirectional RNN**"
      ],
      "metadata": {
        "id": "t1KeRMCvbkA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Bidirectional RNN Model with ReLU activation\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Bidirectional(SimpleRNN(64, activation='relu')))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmQV-X1Ybmel",
        "outputId": "4f5572f3-0e46-458d-d343-c842182bb58e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "140/140 [==============================] - 10s 58ms/step - loss: 0.3104 - accuracy: 0.8847 - val_loss: 0.1593 - val_accuracy: 0.9426\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 0.1382 - accuracy: 0.9585 - val_loss: 0.1231 - val_accuracy: 0.9507\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 8s 55ms/step - loss: 0.0922 - accuracy: 0.9684 - val_loss: 0.1018 - val_accuracy: 0.9623\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 6s 42ms/step - loss: 0.0725 - accuracy: 0.9769 - val_loss: 0.1004 - val_accuracy: 0.9650\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 8s 54ms/step - loss: 0.0567 - accuracy: 0.9838 - val_loss: 0.0996 - val_accuracy: 0.9659\n",
            "35/35 [==============================] - 0s 14ms/step - loss: 0.0996 - accuracy: 0.9659\n",
            "Test Loss: 0.09957161545753479\n",
            "Test Accuracy: 0.9659192562103271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM**"
      ],
      "metadata": {
        "id": "NIgGocKgb_aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Bidirectional LSTM Model with ReLU activation\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Bidirectional(LSTM(64, activation='relu')))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile Model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg39DxhvcBka",
        "outputId": "8d0faa0e-fe94-464a-fdbc-72f6231d8ba2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "140/140 [==============================] - 21s 123ms/step - loss: 42.5086 - accuracy: 0.8914 - val_loss: 0.2561 - val_accuracy: 0.9085\n",
            "Epoch 2/5\n",
            "140/140 [==============================] - 16s 116ms/step - loss: 1.5553 - accuracy: 0.9096 - val_loss: 0.3973 - val_accuracy: 0.8682\n",
            "Epoch 3/5\n",
            "140/140 [==============================] - 17s 119ms/step - loss: 0.3265 - accuracy: 0.8735 - val_loss: 0.4396 - val_accuracy: 0.8735\n",
            "Epoch 4/5\n",
            "140/140 [==============================] - 18s 128ms/step - loss: 0.2496 - accuracy: 0.9161 - val_loss: 0.2901 - val_accuracy: 0.9013\n",
            "Epoch 5/5\n",
            "140/140 [==============================] - 18s 130ms/step - loss: 0.2113 - accuracy: 0.9329 - val_loss: 0.2359 - val_accuracy: 0.9426\n",
            "35/35 [==============================] - 1s 30ms/step - loss: 0.2359 - accuracy: 0.9426\n",
            "Test Loss: 0.2359250783920288\n",
            "Test Accuracy: 0.9426009058952332\n"
          ]
        }
      ]
    }
  ]
}